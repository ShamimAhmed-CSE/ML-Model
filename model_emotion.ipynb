{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db863407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 10:32:13.159004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 10:32:14.232303: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-20 10:32:14.232365: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-20 10:32:17.134529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 10:32:17.134665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 10:32:17.134678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8b2300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train'\n",
    "val_dir = 'test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1013a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 10:32:35.241020: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-08-20 10:32:35.241604: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-20 10:32:35.241629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-08-20 10:32:35.246377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),padding='valid', activation='relu', input_shape=(48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99199fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 46, 46, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 21, 21, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,198,919\n",
      "Trainable params: 2,198,471\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874d09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/tmp/ipykernel_4431/1606634202.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_info = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "448/448 [==============================] - 109s 241ms/step - loss: 1.8572 - accuracy: 0.3405 - val_loss: 1.6910 - val_accuracy: 0.3252\n",
      "Epoch 2/25\n",
      "448/448 [==============================] - 95s 212ms/step - loss: 1.4428 - accuracy: 0.4523 - val_loss: 1.4077 - val_accuracy: 0.4597\n",
      "Epoch 3/25\n",
      "448/448 [==============================] - 103s 229ms/step - loss: 1.3112 - accuracy: 0.5075 - val_loss: 1.3161 - val_accuracy: 0.4951\n",
      "Epoch 4/25\n",
      "448/448 [==============================] - 93s 209ms/step - loss: 1.2048 - accuracy: 0.5505 - val_loss: 1.2901 - val_accuracy: 0.5084\n",
      "Epoch 5/25\n",
      "448/448 [==============================] - 109s 244ms/step - loss: 1.1030 - accuracy: 0.5917 - val_loss: 1.2497 - val_accuracy: 0.5335\n",
      "Epoch 6/25\n",
      "448/448 [==============================] - 97s 218ms/step - loss: 1.0004 - accuracy: 0.6301 - val_loss: 1.2620 - val_accuracy: 0.5226\n",
      "Epoch 7/25\n",
      "448/448 [==============================] - 103s 230ms/step - loss: 0.9055 - accuracy: 0.6739 - val_loss: 1.2114 - val_accuracy: 0.5445\n",
      "Epoch 8/25\n",
      "448/448 [==============================] - 90s 201ms/step - loss: 0.8010 - accuracy: 0.7135 - val_loss: 1.2261 - val_accuracy: 0.5541\n",
      "Epoch 9/25\n",
      "448/448 [==============================] - 90s 201ms/step - loss: 0.7051 - accuracy: 0.7507 - val_loss: 1.2166 - val_accuracy: 0.5593\n",
      "Epoch 10/25\n",
      "448/448 [==============================] - 90s 202ms/step - loss: 0.6154 - accuracy: 0.7891 - val_loss: 1.2533 - val_accuracy: 0.5529\n",
      "Epoch 11/25\n",
      "448/448 [==============================] - 90s 200ms/step - loss: 0.5377 - accuracy: 0.8187 - val_loss: 1.2419 - val_accuracy: 0.5664\n",
      "Epoch 12/25\n",
      "448/448 [==============================] - 90s 201ms/step - loss: 0.4561 - accuracy: 0.8507 - val_loss: 1.3004 - val_accuracy: 0.5632\n",
      "Epoch 13/25\n",
      "448/448 [==============================] - 90s 202ms/step - loss: 0.3957 - accuracy: 0.8733 - val_loss: 1.3273 - val_accuracy: 0.5629\n",
      "Epoch 14/25\n",
      "448/448 [==============================] - 110s 245ms/step - loss: 0.3359 - accuracy: 0.8942 - val_loss: 1.3330 - val_accuracy: 0.5624\n",
      "Epoch 15/25\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.2876 - accuracy: 0.9117 - val_loss: 1.3713 - val_accuracy: 0.5590\n",
      "Epoch 16/25\n",
      "448/448 [==============================] - 113s 251ms/step - loss: 0.2478 - accuracy: 0.9270 - val_loss: 1.4287 - val_accuracy: 0.5555\n",
      "Epoch 17/25\n",
      "448/448 [==============================] - 106s 237ms/step - loss: 0.2180 - accuracy: 0.9373 - val_loss: 1.4570 - val_accuracy: 0.5501\n",
      "Epoch 18/25\n",
      "448/448 [==============================] - 105s 234ms/step - loss: 0.1864 - accuracy: 0.9464 - val_loss: 1.4911 - val_accuracy: 0.5572\n",
      "Epoch 19/25\n",
      "448/448 [==============================] - 95s 213ms/step - loss: 0.1650 - accuracy: 0.9543 - val_loss: 1.5118 - val_accuracy: 0.5573\n",
      "Epoch 20/25\n",
      "448/448 [==============================] - 93s 208ms/step - loss: 0.1483 - accuracy: 0.9586 - val_loss: 1.5542 - val_accuracy: 0.5618\n",
      "Epoch 21/25\n",
      "448/448 [==============================] - 91s 203ms/step - loss: 0.1335 - accuracy: 0.9634 - val_loss: 1.5554 - val_accuracy: 0.5624\n",
      "Epoch 22/25\n",
      "448/448 [==============================] - 91s 203ms/step - loss: 0.1167 - accuracy: 0.9689 - val_loss: 1.5855 - val_accuracy: 0.5590\n",
      "Epoch 23/25\n",
      "448/448 [==============================] - 91s 202ms/step - loss: 0.1101 - accuracy: 0.9707 - val_loss: 1.6867 - val_accuracy: 0.5605\n",
      "Epoch 24/25\n",
      "448/448 [==============================] - 90s 202ms/step - loss: 0.0984 - accuracy: 0.9743 - val_loss: 1.6922 - val_accuracy: 0.5622\n",
      "Epoch 25/25\n",
      "448/448 [==============================] - 90s 201ms/step - loss: 0.0935 - accuracy: 0.9761 - val_loss: 1.7148 - val_accuracy: 0.5598\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "model_info = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16bb3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('model_emotion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be517fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:01:06.248937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 11:01:06.449664: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:06.449685: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-24 11:01:07.416073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:07.416196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:07.416211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-24 11:01:08.531158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:08.531192: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-24 11:01:08.531217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-06-24 11:01:08.531698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load trained model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "emotion_model = load_model(\"model73.h5\")\n",
    "\n",
    "# prevents openCL usage and unnecessary logging messages\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# dictionary mapping class labels with corresponding emotions\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break;\n",
    "        \n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    facecasc = cv2.CascadeClassifier('/home/shamim/Downloads/haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Video', cv2.resize(frame,(500,500),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 11:35:21.155598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 11:35:21.323171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-20 11:35:21.323194: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-20 11:35:22.341632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 11:35:22.341734: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 11:35:22.341745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for emotion detaction and 2 for upscaling1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 11:35:26.843844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-08-20 11:35:26.843900: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-20 11:35:26.843956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-08-20 11:35:26.844624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "n =int(input(\"Enter 1 for emotion detaction and 2 for upscaling\"))\n",
    "\n",
    "if n == 1:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from keras.models import load_model\n",
    "\n",
    "    # Load trained model\n",
    "    emotion_model = load_model(\"model_emotion.h5\")\n",
    "\n",
    "    # Dictionary mapping class labels with corresponding emotions\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread('smile2.jpg')\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    face_cascade = cv2.CascadeClassifier('/home/shamim/Downloads/haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        max_index = int(np.argmax(prediction))\n",
    "        emotion_label = emotion_dict[max_index]\n",
    "        cv2.putText(image, emotion_label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the image\n",
    "    cv2.imshow('Emotion Detection', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "elif n==2:\n",
    "    img = cv2.imread('smile2.jpg')\n",
    " \n",
    "    image_plot = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.title(image_plot.shape)\n",
    "    plt.imshow(image_plot)\n",
    "    plt.show()\n",
    " \n",
    "    # Running the SR model\n",
    " \n",
    "    # Model to preprocess the images\n",
    " \n",
    " \n",
    "    def preprocessing(img):\n",
    "        imageSize = (tf.convert_to_tensor(image_plot.shape[:-1]) // 4) * 4\n",
    "        cropped_image = tf.image.crop_to_bounding_box(\n",
    "            img, 0, 0, imageSize[0], imageSize[1])\n",
    "        preprocessed_image = tf.cast(cropped_image, tf.float32)\n",
    "        return tf.expand_dims(preprocessed_image, 0)\n",
    " \n",
    " \n",
    "    # This is a model of Enhanced Super Resolution GAN Model\n",
    "    # The link given here is a model of ESRGAN model\n",
    "    esrgn_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\n",
    "    model = hub.load(esrgn_path)\n",
    " \n",
    "    # Model to employ the model\n",
    " \n",
    " \n",
    "    def srmodel(img):\n",
    "        preprocessed_image = preprocessing(img)  # Preprocess the LR Image\n",
    "        new_image = model(preprocessed_image)  # Runs the model\n",
    "        # returns the size of the original argument that is given as input\n",
    "        return tf.squeeze(new_image) / 255.0\n",
    " \n",
    " \n",
    "    # Plot the HR image\n",
    "    hr_image = srmodel(image_plot)\n",
    "    plt.title(hr_image.shape)\n",
    "    plt.imshow(hr_image)\n",
    "    plt.show()\n",
    "\n",
    "else: \n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Your emotion_dict\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# Assuming you have already trained your model and have predictions\n",
    "# and true labels for the validation set.\n",
    "predictions = model.predict(validation_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = validation_generator.classes\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot non-normalized confusion matrix with emotion labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cm, classes=emotion_dict.values(), title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('model_emotion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
